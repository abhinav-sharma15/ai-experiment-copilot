{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dac3781-faeb-4e43-b01e-a2f1c3b6e1e4",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Model: the code uses Ollama (llama3 by default). Pull it once: ollama pull llama3.\n",
    "\n",
    "RAG: uses lightweight TF-IDF (scikit-learn)\n",
    "\n",
    "Outputs: saved to outputs/summary.json, outputs/suggestions.json, and optional batch artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502dd7d8-81f3-4592-aa35-74a6c79c8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Install light dependencies\n",
    "\n",
    "# If you already have these, you can skip.\n",
    "# Keeps it light: NO pyarrow, NO Chroma, NO transformers.\n",
    "!pip install -q pandas==2.2.2 scipy==1.11.4 scikit-learn==1.5.2 langchain==0.2.10 langchain-core==0.2.10 langchain-ollama==0.1.0 pydantic==2.7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cb980b-7e50-42fd-bc9f-ff608bea4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/abhinavsharma/Documents/copilot_mvp\n",
      "DATA: /Users/abhinavsharma/Documents/copilot_mvp/data\n",
      "RAG: /Users/abhinavsharma/Documents/copilot_mvp/rag\n",
      "OUT: /Users/abhinavsharma/Documents/copilot_mvp/outputs\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths and helpers\n",
    "\n",
    "import os, json, math, re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "# Project dirs\n",
    "ROOT = Path.cwd()\n",
    "DATA = ROOT / \"data\"\n",
    "RAGDIR = ROOT / \"rag\"\n",
    "OUT = ROOT / \"outputs\"\n",
    "OUT.mkdir(exist_ok=True, parents=True)\n",
    "RAGDIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"RAG:\", RAGDIR)\n",
    "print(\"OUT:\", OUT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fb14a5-ac65-4681-931b-9a131d3d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats: z-test for proportions + verdict rules\n",
    "\n",
    "from math import erf, sqrt\n",
    "\n",
    "@dataclass\n",
    "class ZTestResult:\n",
    "    control_rate: float\n",
    "    treatment_rate: float\n",
    "    lift: float\n",
    "    p_value: float\n",
    "    ci_low: float\n",
    "    ci_high: float\n",
    "\n",
    "def _phi(z):  # CDF of standard normal\n",
    "    return 0.5 * (1.0 + erf(z / sqrt(2.0)))\n",
    "\n",
    "def _z_from_alpha_two_tailed(alpha: float) -> float:\n",
    "    # binary search inverse CDF\n",
    "    lo, hi = -10.0, 10.0\n",
    "    target = 1 - alpha/2\n",
    "    for _ in range(80):\n",
    "        mid = (lo + hi) / 2\n",
    "        if _phi(mid) < target: lo = mid\n",
    "        else: hi = mid\n",
    "    return (lo + hi) / 2\n",
    "\n",
    "def z_test_proportions(n1:int, x1:int, n2:int, x2:int, alpha=0.05) -> ZTestResult:\n",
    "    p1 = x1 / max(n1, 1)\n",
    "    p2 = x2 / max(n2, 1)\n",
    "    lift = (p2 - p1)\n",
    "    # pooled std error\n",
    "    p_pool = (x1 + x2) / max(n1 + n2, 1)\n",
    "    se = sqrt(p_pool*(1-p_pool)*(1/n1 + 1/n2)) if n1>0 and n2>0 else 1e9\n",
    "    z = 0 if se==0 else (p2 - p1) / se\n",
    "    pval = 2 * (1 - _phi(abs(z)))\n",
    "    # CI around difference\n",
    "    zcrit = _z_from_alpha_two_tailed(alpha)\n",
    "    ci_low, ci_high = lift - zcrit*se, lift + zcrit*se\n",
    "    return ZTestResult(p1, p2, lift, pval, ci_low, ci_high)\n",
    "\n",
    "def verdict_from_result(res: ZTestResult, alpha=0.05, min_abs_lift=0.0) -> str:\n",
    "    if res.p_value < alpha and abs(res.lift) > min_abs_lift:\n",
    "        return \"Win\" if res.lift > 0 else \"Lose\"\n",
    "    return \"Inconclusive\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f969da45-3594-49ac-9efc-622e556dfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV loader (expects: variant, n, conversions, [experiemnt_name, hypothesis, primary_metric])\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    name: str\n",
    "    hypothesis: str\n",
    "    primary_metric: str\n",
    "    rows: List[Dict[str, Any]]\n",
    "\n",
    "def load_experiment_csv(csv_path: str) -> Experiment:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    req = {\"variant\",\"n\",\"conversions\"}\n",
    "    assert req.issubset(set(df.columns)), f\"CSV must have columns: {req}\"\n",
    "    # best-effort meta\n",
    "    name = (df[\"experiment_name\"].iloc[0] if \"experiment_name\" in df.columns else Path(csv_path).stem)\n",
    "    hypothesis = (df[\"hypothesis\"].iloc[0] if \"hypothesis\" in df.columns else \"N/A\")\n",
    "    primary = (df[\"primary_metric\"].iloc[0] if \"primary_metric\" in df.columns else \"conversion_rate\")\n",
    "\n",
    "    # Normalise variants to control/treatment order\n",
    "    variants = list(df[\"variant\"].astype(str).str.lower().unique())\n",
    "    # heuristic: control first if present\n",
    "    if \"control\" in variants:\n",
    "        order = [\"control\"] + [v for v in variants if v != \"control\"]\n",
    "    else:\n",
    "        order = variants\n",
    "    rows = []\n",
    "    for v in order:\n",
    "        sub = df[df[\"variant\"].astype(str).str.lower() == v].iloc[0]\n",
    "        rows.append({\"variant\": v, \"n\": int(sub[\"n\"]), \"x\": int(sub[\"conversions\"])})\n",
    "    return Experiment(name=name, hypothesis=hypothesis, primary_metric=primary, rows=rows)\n",
    "\n",
    "def build_stats_json(rows: List[Dict[str, Any]], metric_name:str) -> Tuple[Dict[str,Any], ZTestResult]:\n",
    "    assert len(rows) >= 2, \"Need at least two variants (control + treatment).\"\n",
    "    c, t = rows[0], rows[1]\n",
    "    res = z_test_proportions(c[\"n\"], c[\"x\"], t[\"n\"], t[\"x\"], alpha=0.05)\n",
    "    payload = {\n",
    "        \"metric\": metric_name,\n",
    "        \"baseline\": {\"variant\": rows[0][\"variant\"], \"n\": c[\"n\"], \"x\": c[\"x\"], \"rate\": res.control_rate},\n",
    "        \"treatment\": {\"variant\": rows[1][\"variant\"], \"n\": t[\"n\"], \"x\": t[\"x\"], \"rate\": res.treatment_rate},\n",
    "        \"lift\": res.lift,\n",
    "        \"p_value\": res.p_value,\n",
    "        \"ci\": [res.ci_low, res.ci_high]\n",
    "    }\n",
    "    return payload, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e345636-ad64-4a28-888b-629194e4f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG: read rag/past_experiemnts.jsonl + TF-IDF retriever\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_past_experiments(jsonl_path: Path) -> List[Dict[str, Any]]:\n",
    "    if not jsonl_path.exists():\n",
    "        # bootstrap with a couple of examples if file missing\n",
    "        samples = [\n",
    "            {\"id\":\"exp_101\",\"name\":\"Signup CTA Contrast\",\"date\":\"2025-07-12\",\"metric\":\"signup_rate\",\"verdict\":\"Win\",\n",
    "             \"summary\":\"Higher contrast improved clarity on mobile; negligible effect on desktop.\",\n",
    "             \"learnings\":[\"Mobile users responded to contrast\",\"Desktop insensitive to CTA color\"],\n",
    "             \"context\":\"Unauthenticated traffic\"},\n",
    "            {\"id\":\"exp_102\",\"name\":\"Checkout Short Copy\",\"date\":\"2025-08-02\",\"metric\":\"completion_rate\",\"verdict\":\"Lose\",\n",
    "             \"summary\":\"Shorter copy reduced trust for first-time users.\",\n",
    "             \"learnings\":[\"Loss on new users\",\"Better for returning users\"],\n",
    "             \"context\":\"Checkout step 2\"}\n",
    "        ]\n",
    "        with open(jsonl_path, \"w\") as f:\n",
    "            for s in samples: f.write(json.dumps(s)+\"\\n\")\n",
    "    docs = []\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                r = json.loads(line)\n",
    "                body = (\n",
    "                    f\"Name: {r.get('name','')}\\nVerdict: {r.get('verdict','')}\\n\"\n",
    "                    f\"Metric: {r.get('metric','')}\\nSummary: {r.get('summary','')}\\n\"\n",
    "                    f\"Learnings: {', '.join(r.get('learnings', []))}\\nContext: {r.get('context','')}\"\n",
    "                )\n",
    "                docs.append({\"id\": r.get(\"id\",\"\"), \"text\": body})\n",
    "            except Exception:\n",
    "                continue\n",
    "    return docs\n",
    "\n",
    "class TfidfRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.vectorizer = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "        self.matrix = self.vectorizer.fit_transform(texts)\n",
    "        self.texts = texts\n",
    "\n",
    "    def top_k(self, query: str, k=5) -> List[Tuple[str, float]]:\n",
    "        qv = self.vectorizer.transform([query])\n",
    "        sims = cosine_similarity(qv, self.matrix).ravel()\n",
    "        idxs = sims.argsort()[::-1][:k]\n",
    "        return [(self.texts[i], float(sims[i])) for i in idxs]\n",
    "\n",
    "def build_retriever(jsonl_path=RAGDIR / \"past_experiments.jsonl\") -> Tuple[TfidfRetriever, List[Dict[str,Any]]]:\n",
    "    docs = load_past_experiments(jsonl_path)\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    retriever = TfidfRetriever(texts) if texts else None\n",
    "    return retriever, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a473a5-4bd9-4220-9ea2-acc78ed6e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chains (Ollama) with robust JSON parsing + RAG context\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# ---- Schemas\n",
    "class MetricResult(BaseModel):\n",
    "    metric: str\n",
    "    baseline_value: float\n",
    "    treatment_value: float\n",
    "    lift: float\n",
    "    p_value: float\n",
    "    ci: List[float]  # [low, high]\n",
    "\n",
    "class ExperimentSummary(BaseModel):\n",
    "    title: str\n",
    "    verdict: str\n",
    "    summary: str\n",
    "    key_metrics: List[MetricResult]\n",
    "    drivers: List[str]\n",
    "    caveats: List[str]\n",
    "    recommended_actions: List[str]\n",
    "\n",
    "class TestIdea(BaseModel):\n",
    "    title: str\n",
    "    hypothesis: str\n",
    "    rationale: str\n",
    "    metrics: List[str]\n",
    "    sample_size_hint: Optional[str] = None\n",
    "    risks: List[str]\n",
    "    instrumentation: List[str]\n",
    "\n",
    "class Suggestions(BaseModel):\n",
    "    overall_theme: str\n",
    "    ideas: List[TestIdea]\n",
    "\n",
    "STRICT = (\n",
    "    \"Return ONLY valid JSON per the schema. No markdown/code fences/prose. \"\n",
    "    \"Numbers must be plain JSON numbers (no NaN/Infinity). No extra fields.\"\n",
    ")\n",
    "\n",
    "# ---- Chains\n",
    "# ---- Chains: return (text_pipe, parser) so we can do safe parsing\n",
    "def build_summarize_chain(model_name=\"llama3\"):\n",
    "    llm = OllamaLLM(model=model_name, temperature=0.1)\n",
    "    parser = PydanticOutputParser(pydantic_object=ExperimentSummary)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an experimentation analyst. Be precise and actionable. \" + STRICT),\n",
    "        (\"human\",\n",
    "         \"Experiment: {name}\\nHypothesis: {hypothesis}\\nPrimary metric: {primary_metric}\\n\"\n",
    "         \"Stats (json): {stats_json}\\nSegments (json): {segments_json}\\n\\n\"\n",
    "         \"Write a structured summary.\\n{format_instructions}\")\n",
    "    ]).partial(format_instructions=parser.get_format_instructions())\n",
    "    text_pipe = prompt | llm      # stop before parser\n",
    "    return text_pipe, parser\n",
    "\n",
    "def build_suggestions_chain(model_name=\"llama3\"):\n",
    "    llm = OllamaLLM(model=model_name, temperature=0.2)\n",
    "    parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Propose practical next tests grounded in outcome and provided RAG snippets. \" + STRICT),\n",
    "        (\"human\",\n",
    "         \"Heuristics: {heuristics}\\nOutcome: {verdict}\\nKey learnings: {learnings}\\nConstraints: {constraints}\\n\"\n",
    "         \"Experiment: {name}\\nHypothesis: {hypothesis}\\n\\n\"\n",
    "         \"Relevant past learnings (use as evidence, quote sparingly):\\n{kb_snippets}\\n\\n\"\n",
    "         \"Propose exactly three ideas with rationale and one instrumentation suggestion.\\n{format_instructions}\")\n",
    "    ]).partial(format_instructions=parser.get_format_instructions())\n",
    "    text_pipe = prompt | llm\n",
    "    return text_pipe, parser\n",
    "\n",
    "def verdict_emoji(v: str) -> str:\n",
    "    return {\"Win\":\"âœ…\", \"Lose\":\"âŒ\", \"Inconclusive\":\"ðŸŸ¨\"}.get(v, \"â„¹ï¸\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a56b8a6-46a1-4508-a530-1afa31076966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Safe JSON utilities (no RetryOutputParser needed)\n",
    "import re, json\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    # remove ```json ... ``` or ``` ... ```\n",
    "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", s.strip(), flags=re.I|re.M)\n",
    "\n",
    "def _extract_json_block(s: str) -> str:\n",
    "    s = _strip_code_fences(s)\n",
    "    # find first balanced {...} or [...]\n",
    "    # simple bracket counter to avoid greedy regex traps\n",
    "    for open_ch, close_ch in [(\"{\", \"}\"), (\"[\", \"]\")]:\n",
    "        start = s.find(open_ch)\n",
    "        if start != -1:\n",
    "            depth = 0\n",
    "            for i, ch in enumerate(s[start:], start=start):\n",
    "                if ch == open_ch: depth += 1\n",
    "                elif ch == close_ch: depth -= 1\n",
    "                if depth == 0:\n",
    "                    return s[start:i+1]\n",
    "    # fallback: try the first JSON-looking block with regex\n",
    "    m = re.search(r'(\\{.*\\}|\\[.*\\])', s, flags=re.S)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "def run_llm_to_text(pipe, inputs: Dict[str, Any]) -> str:\n",
    "    \"\"\"Invoke a (prompt|llm) pipeline and return raw string content.\"\"\"\n",
    "    msg = pipe.invoke(inputs)\n",
    "    # langchain message objects often have .content; fall back to str\n",
    "    return getattr(msg, \"content\", str(msg))\n",
    "\n",
    "def safe_invoke(pipe, parser, inputs: Dict[str, Any], max_retries: int = 2):\n",
    "    \"\"\"Run LLM -> raw text -> extract JSON -> parse with Pydantic, with retries & repair.\"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        raw = run_llm_to_text(pipe, inputs)\n",
    "        try:\n",
    "            # first try as-is\n",
    "            return parser.parse(raw)\n",
    "        except Exception as e1:\n",
    "            # try with code-fence and block extraction\n",
    "            try:\n",
    "                cleaned = _extract_json_block(raw)\n",
    "                return parser.parse(cleaned)\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "                # small guidance tweak for the next retry\n",
    "                inputs = dict(inputs)\n",
    "                inputs[\"format_instructions\"] = parser.get_format_instructions() + \\\n",
    "                    \"\\nReturn ONLY the JSON object. No prose. No code fences.\"\n",
    "                continue\n",
    "    # bubble up a compact hint for debugging\n",
    "    raise type(last_err)(f\"{last_err}\\n--- Raw LLM output snippet ---\\n{raw[:800]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047610cf-49cf-42a2-9f91-bd45326cb1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/k2gk5s4s2nv619kgh3w46zyw0000gn/T/ipykernel_8971/634704735.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  summary = summary if isinstance(summary, dict) else summary.dict()\n",
      "/var/folders/8s/k2gk5s4s2nv619kgh3w46zyw0000gn/T/ipykernel_8971/634704735.py:48: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  suggestions = suggestions if isinstance(suggestions, dict) else suggestions.dict()\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### âœ… Experiment: Japan Pricing Test: actual vs 30% discount\n",
       "**Verdict:** Win  \n",
       "**Primary metric:** conversion_rate  \n",
       "**Summary:** Providing a 30% discount led to a significant increase in conversion rate and ultimately more billings.\n",
       "\n",
       "**Key metric:** conversion_rate\n",
       "- Control: 0.0016507841224581677\n",
       "- Treatment: 0.003986243551664843\n",
       "- Lift: 0.002335459429206675\n",
       "- p-value: 0.00035119951427731166\n",
       "- CI: [0.001055, 0.003616]\n",
       "\n",
       "**Top Actions:**\n",
       "- \n",
       "- \n",
       "- \n",
       "\n",
       "**Next Test Ideas:** Copy/Contrast Test, Discount Ladder Test, Price Anchoring Test\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main runner (single experiment) with RAG\n",
    "\n",
    "# ---- Build retriever once (loads or bootstraps rag/past_experiments.jsonl)\n",
    "retriever, rag_docs = build_retriever()\n",
    "\n",
    "def run_one(csv_path: str, model_name=\"llama3\", constraints=\"Low eng bandwidth next sprint\") -> Dict[str, Any]:\n",
    "    exp = load_experiment_csv(csv_path)\n",
    "    stats_json, res = build_stats_json(exp.rows, metric_name=exp.primary_metric)\n",
    "    verdict = verdict_from_result(res)\n",
    "\n",
    "    # RAG: use experiment name + hypothesis + metric as query\n",
    "    kb_snips = \"\"\n",
    "    if retriever:\n",
    "        q = f\"{exp.name} {exp.hypothesis} {exp.primary_metric}\"\n",
    "        tops = retriever.top_k(q, k=5)\n",
    "        kb_snips = \"\\n---\\n\".join([t for (t,score) in tops])\n",
    "\n",
    "        sum_pipe, sum_parser = build_summarize_chain(model_name)\n",
    "        summary = safe_invoke(sum_pipe, sum_parser, {\n",
    "            \"name\": exp.name,\n",
    "            \"hypothesis\": exp.hypothesis,\n",
    "            \"primary_metric\": exp.primary_metric,\n",
    "            \"stats_json\": json.dumps(stats_json, default=float),\n",
    "            \"segments_json\": \"[]\"\n",
    "        })\n",
    "        summary = summary if isinstance(summary, dict) else summary.dict()\n",
    "        \n",
    "        # pick heuristics with a safe default\n",
    "        DEFAULT_HEURISTICS = \"\"\"\n",
    "        - If Win but lift < 5%, recommend cheap iteration (copy/contrast/placement).\n",
    "        - If Lose, consider rollback, alternative framing, or segmentation.\n",
    "        - If Inconclusive, extend run or increase sample; consider higher-signal proxy.\n",
    "        - Always include an instrumentation improvement.\n",
    "        - Pricing-specific: discount ladders, price anchoring, strike-through formatting, margin guardrails.\n",
    "        \"\"\"\n",
    "        H = globals().get(\"HEURISTICS\", DEFAULT_HEURISTICS)\n",
    "\n",
    "        sug_pipe, sug_parser = build_suggestions_chain(model_name)\n",
    "        suggestions = safe_invoke(sug_pipe, sug_parser, {\n",
    "            \"heuristics\": H,\n",
    "            \"verdict\": verdict,\n",
    "            \"learnings\": \"; \".join(summary.get(\"drivers\", [])),\n",
    "            \"constraints\": constraints,\n",
    "            \"name\": exp.name,\n",
    "            \"hypothesis\": exp.hypothesis,\n",
    "            \"kb_snippets\": kb_snips\n",
    "        })\n",
    "        suggestions = suggestions if isinstance(suggestions, dict) else suggestions.dict()\n",
    "\n",
    "\n",
    "    bundle = {\n",
    "        \"csv\": csv_path,\n",
    "        \"experiment\": {\"name\": exp.name, \"hypothesis\": exp.hypothesis, \"primary_metric\": exp.primary_metric},\n",
    "        \"stats\": stats_json,\n",
    "        \"verdict\": verdict,\n",
    "        \"summary\": summary,\n",
    "        \"suggestions\": suggestions\n",
    "    }\n",
    "    # Save outputs\n",
    "    with open(OUT / \"summary.json\", \"w\") as f: json.dump(summary, f, indent=2)\n",
    "    with open(OUT / \"suggestions.json\", \"w\") as f: json.dump(suggestions, f, indent=2)\n",
    "    return bundle\n",
    "\n",
    "def format_md(bundle: Dict[str,Any]) -> str:\n",
    "    s = bundle[\"summary\"]; exp = bundle[\"experiment\"]; v = bundle[\"verdict\"]\n",
    "    km = (s.get(\"key_metrics\") or [{}])[0]\n",
    "    ci = km.get(\"ci\", \"N/A\")\n",
    "    ci_txt = \"[\" + \", \".join(f\"{x:.6f}\" for x in ci) + \"]\" if isinstance(ci, list) else str(ci)\n",
    "    actions = s.get(\"recommended_actions\", [])[:3]\n",
    "    ideas = [i.get(\"title\",\"\") for i in bundle[\"suggestions\"].get(\"ideas\", [])]\n",
    "    return f\"\"\"### {verdict_emoji(v)} {s.get('title', exp['name'])}\n",
    "**Verdict:** {v}  \n",
    "**Primary metric:** {exp['primary_metric']}  \n",
    "**Summary:** {s.get('summary','(no summary)')}\n",
    "\n",
    "**Key metric:** {km.get('metric','N/A')}\n",
    "- Control: {km.get('baseline_value','N/A')}\n",
    "- Treatment: {km.get('treatment_value','N/A')}\n",
    "- Lift: {km.get('lift','N/A')}\n",
    "- p-value: {km.get('p_value','N/A')}\n",
    "- CI: {ci_txt}\n",
    "\n",
    "**Top Actions:**\n",
    "- {(actions[0] if len(actions)>0 else '')}\n",
    "- {(actions[1] if len(actions)>1 else '')}\n",
    "- {(actions[2] if len(actions)>2 else '')}\n",
    "\n",
    "**Next Test Ideas:** {\", \".join(ideas) if ideas else \"(none)\"}\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE RUN (change the CSV path as needed)\n",
    "csv_path = str(DATA / \"JP_pricing_test.csv\")  # e.g., data/sample_ab_win.csv\n",
    "bundle = run_one(csv_path, model_name=\"llama3\")\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(format_md(bundle)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba01af6-99ff-472e-8b38-6ea784c043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Batch: run all CSVs in/data and write per-test markdown\n",
    "\n",
    "def run_batch(data_dir=DATA, model_name=\"llama3\"):\n",
    "    reports = OUT / \"reports_rag\"\n",
    "    reports.mkdir(exist_ok=True, parents=True)\n",
    "    results = []\n",
    "    for p in sorted(Path(data_dir).glob(\"*.csv\")):\n",
    "        try:\n",
    "            b = run_one(str(p), model_name=model_name)\n",
    "            results.append(b)\n",
    "            with open(reports / f\"{p.stem}.md\",\"w\") as f:\n",
    "                f.write(format_md(b))\n",
    "            print(\"OK ->\", p.name)\n",
    "        except Exception as e:\n",
    "            print(\"FAIL ->\", p.name, e)\n",
    "    with open(OUT / \"batch.jsonl\",\"w\") as f:\n",
    "        for r in results: f.write(json.dumps(r)+\"\\n\")\n",
    "    print(\"Saved:\", OUT / \"batch.jsonl\")\n",
    "    return results\n",
    "\n",
    "# Uncomment to run all:\n",
    "# batch = run_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40b149-55da-4691-af90-d2184642288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG file format (edit as you like)\n",
    "# Create/edit rag/past_experiemnts.jsonl (one json per line):\n",
    "\n",
    "{\"id\":\"exp_201\",\"name\":\"Pricing Banner Trust Badges\",\"date\":\"2025-09-01\",\"metric\":\"checkout_completion\",\"verdict\":\"Win\",\"summary\":\"Trust badges increased completion for first-time buyers.\",\"learnings\":[\"First-time users need reassurance\",\"Badges near payment form worked best\"],\"context\":\"Checkout step 3\"}\n",
    "{\"id\":\"exp_202\",\"name\":\"Hero Copy Shortening\",\"date\":\"2025-09-18\",\"metric\":\"signup_rate\",\"verdict\":\"Lose\",\"summary\":\"Shorter copy reduced clarity for cold traffic.\",\"learnings\":[\"Cold traffic prefers clarity over brevity\"],\"context\":\"Landing page\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb44876-6dd7-4418-9bd4-43d9a87813e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
